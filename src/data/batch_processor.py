#!/usr/bin/env python3
"""
Batch Embedding Service for GPU-accelerated query processing.

This module provides async batch processing for embedding queries, supporting:
- Apple Silicon Metal acceleration (M1/M2/M3 Macs)
- NVIDIA CUDA acceleration
- CPU fallback with optimizations
- Concurrent query handling with 2-5x performance improvement
"""

import asyncio
import platform
import time
import uuid
from dataclasses import dataclass
from typing import Any, Dict, List, Optional

import numpy as np

try:
    import torch
    from sentence_transformers import SentenceTransformer

    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    SentenceTransformer = None

from src.core.embeddings import create_embeddings, get_embedding_model


@dataclass
class QueryRequest:
    """Represents a single query embedding request"""

    id: str
    query: str
    timestamp: float
    callback: Optional[asyncio.Future] = None


class BatchEmbeddingService:
    """
    Async service for batch processing of query embeddings.

    Features:
    - Automatic hardware detection (Metal/CUDA/CPU)
    - Dynamic batch sizing based on load
    - Result distribution to individual requesters
    - Performance monitoring and optimization
    """

    def __init__(
        self,
        model_name: str = "embeddinggemma:latest",
        backend: str = "ollama",
        max_batch_size: int = 16,
    ):
        self.model_name = model_name
        self.backend = backend
        self.max_batch_size = max_batch_size
        self.model = None
        self.device = self._detect_optimal_device()
        self.queue = asyncio.Queue()
        self.results: Dict[str, np.ndarray] = {}
        self.is_running = False
        self.stats = {
            "total_queries": 0,
            "batch_count": 0,
            "avg_batch_size": 0.0,
            "avg_processing_time": 0.0,
            "gpu_utilization": 0.0,
        }

        # Initialize model
        self._initialize_model()

        # Start background processor
        self.processor_task = None

    def _detect_optimal_device(self) -> str:
        """Detect the optimal device for embedding processing"""
        if not TORCH_AVAILABLE:
            return "cpu"

        # Check for Apple Silicon (Metal)
        system = platform.system()
        machine = platform.machine()
        if system == "Darwin" and machine.startswith("arm64"):
            if torch.backends.mps.is_available():
                print("‚úÖ Detected Apple Silicon - using Metal acceleration")
                return "mps"

        # Check for CUDA
        if torch.cuda.is_available():
            print("‚úÖ Detected NVIDIA GPU - using CUDA acceleration")
            return "cuda"

        # Fallback to CPU
        print("‚ÑπÔ∏è Using CPU processing (GPU acceleration not available)")
        return "cpu"

    def _initialize_model(self):
        """Initialize the embedding model with optimal settings"""
        try:
            # Use the embedding backend system
            self.model = get_embedding_model(self.model_name, self.backend)

            # Device-specific optimizations (only for sentence-transformers backend)
            if self.backend == "sentence-transformers" and TORCH_AVAILABLE:
                if self.device == "mps":
                    # Metal-specific optimizations for Apple Silicon
                    self.max_batch_size = min(
                        self.max_batch_size, 8
                    )  # Smaller batches for unified memory
                    torch.mps.set_per_process_memory_fraction(0.8)  # Reserve 20% for system
                    print("üîß Applied Metal optimizations for Apple Silicon")
                elif self.device == "cuda":
                    # CUDA optimizations
                    self.max_batch_size = 16  # Larger batches for GPU memory
                    if torch.cuda.is_available():
                        torch.cuda.set_per_process_memory_fraction(0.9)  # Use 90% of GPU memory
                    print("üîß Applied CUDA optimizations for NVIDIA GPU")

            print(
                f"‚úÖ Batch embedding service initialized on {self.device.upper()} with {self.backend} backend"
            )

        except Exception as e:
            print(f"‚ùå Failed to initialize embedding model: {e}")
            raise

    async def start_processing(self):
        """Start the background batch processing task"""
        if self.is_running:
            return

        self.is_running = True
        self.processor_task = asyncio.create_task(self._process_batches())
        print("üöÄ Batch embedding processor started")

    async def stop_processing(self):
        """Stop the background batch processing task"""
        self.is_running = False
        if self.processor_task:
            self.processor_task.cancel()
            try:
                await self.processor_task
            except asyncio.CancelledError:
                pass
            self.processor_task = None
        print("üõë Batch embedding processor stopped")

    async def embed_query_async(self, query: str) -> np.ndarray:
        """
        Async interface for embedding a single query.
        Returns when the embedding is ready.
        """
        request_id = str(uuid.uuid4())
        request = QueryRequest(id=request_id, query=query, timestamp=time.time())

        # Create a future for the result
        future = asyncio.Future()
        request.callback = future

        # Add to queue
        await self.queue.put(request)

        # Wait for result
        return await future

    def embed_query_sync(self, query: str) -> np.ndarray:
        """
        Synchronous wrapper for single query embedding.
        Creates a new event loop if needed.
        """
        try:
            # Try to get existing event loop
            loop = asyncio.get_event_loop()
            if loop.is_running():
                # If loop is already running, we need to handle this differently
                # For now, fall back to direct embedding
                return self._embed_single_query(query)
            else:
                return loop.run_until_complete(self.embed_query_async(query))
        except RuntimeError:
            # No event loop, create one
            return asyncio.run(self.embed_query_async(query))

    def _embed_single_query(self, query: str) -> np.ndarray:
        """Direct embedding for fallback cases"""
        embeddings, _ = create_embeddings(
            [query],
            model_name=self.model_name,
            backend=self.backend,
            batch_size=1,
            show_progress=False,
            use_cache=False,
        )
        if embeddings is not None:
            return embeddings[0]
        else:
            return np.zeros(768)  # fallback

    async def _process_batches(self):
        """Main batch processing loop"""
        while self.is_running:
            try:
                # Collect a batch of queries
                batch = await self._collect_batch()

                if not batch:
                    # No queries, wait a bit
                    await asyncio.sleep(0.01)
                    continue

                # Process the batch
                start_time = time.time()
                embeddings = await self._embed_batch(batch)
                processing_time = time.time() - start_time

                # Update statistics
                self._update_stats(len(batch), processing_time)

                # Distribute results
                await self._distribute_results(batch, embeddings)

            except Exception as e:
                print(f"‚ùå Batch processing error: {e}")
                # Continue processing despite errors
                await asyncio.sleep(0.1)

    async def _collect_batch(self) -> List[QueryRequest]:
        """Collect a batch of queries from the queue"""
        batch = []
        batch_start_time = time.time()

        # Try to fill the batch
        while len(batch) < self.max_batch_size:
            try:
                # Wait for first query with timeout
                if not batch:
                    request = await asyncio.wait_for(
                        self.queue.get(),
                        timeout=0.05,  # 50ms max wait for first query
                    )
                    batch.append(request)
                else:
                    # Try to get more queries without waiting
                    try:
                        request = self.queue.get_nowait()
                        batch.append(request)
                    except asyncio.QueueEmpty:
                        break

            except asyncio.TimeoutError:
                # No queries available, return what we have
                break

            # Check if we've waited too long for this batch
            if time.time() - batch_start_time > 0.1:  # Max 100ms batch collection
                break

        return batch

    async def _embed_batch(self, batch: List[QueryRequest]) -> np.ndarray:
        """Embed a batch of queries"""
        if not batch:
            return np.array([])

        queries = [req.query for req in batch]

        try:
            # Use the embedding backend system
            embeddings, _ = create_embeddings(
                queries,
                model_name=self.model_name,
                backend=self.backend,
                batch_size=len(queries),
                show_progress=False,
                use_cache=False,
            )
            if embeddings is None:
                raise ValueError("Embedding creation returned None")
            return embeddings

        except Exception as e:
            print(f"‚ùå Batch embedding failed: {e}")
            # Fallback to individual processing
            embeddings = []
            for query in queries:
                try:
                    single_embeddings, _ = create_embeddings(
                        [query],
                        model_name=self.model_name,
                        backend=self.backend,
                        batch_size=1,
                        show_progress=False,
                        use_cache=False,
                    )
                    if single_embeddings is not None:
                        embeddings.append(single_embeddings[0])
                    else:
                        embeddings.append(np.zeros(768))
                except Exception as e2:
                    print(f"‚ùå Individual embedding failed for query: {e2}")
                    # Return zero vector as fallback
                    embeddings.append(np.zeros(768))  # nomic-embed dimension

            return np.array(embeddings)

    async def _distribute_results(self, batch: List[QueryRequest], embeddings: np.ndarray):
        """Distribute batch results to individual requesters"""
        for i, request in enumerate(batch):
            if i < len(embeddings):
                embedding = embeddings[i]
                self.results[request.id] = embedding

                # Set the result on the future
                if request.callback and not request.callback.done():
                    request.callback.set_result(embedding)

    def _update_stats(self, batch_size: int, processing_time: float):
        """Update performance statistics"""
        self.stats["total_queries"] += batch_size
        self.stats["batch_count"] += 1

        # Rolling average for batch size
        current_avg = self.stats["avg_batch_size"]
        self.stats["avg_batch_size"] = (
            current_avg * (self.stats["batch_count"] - 1) + batch_size
        ) / self.stats["batch_count"]

        # Rolling average for processing time
        current_avg_time = self.stats["avg_processing_time"]
        self.stats["avg_processing_time"] = (
            current_avg_time * (self.stats["batch_count"] - 1) + processing_time
        ) / self.stats["batch_count"]

        # Estimate GPU utilization (rough approximation)
        if self.device in ["cuda", "mps"]:
            self.stats["gpu_utilization"] = min(0.9, batch_size / self.max_batch_size)

    def get_stats(self) -> Dict[str, Any]:
        """Get current performance statistics"""
        return {
            **self.stats,
            "device": self.device,
            "max_batch_size": self.max_batch_size,
            "queue_size": self.queue.qsize(),
            "is_running": self.is_running,
        }

    async def health_check(self) -> bool:
        """Check if the batch service is healthy"""
        try:
            # Try a simple embedding
            test_embedding = await self.embed_query_async("test")
            return len(test_embedding) == 768  # nomic-embed dimension
        except Exception:
            return False
