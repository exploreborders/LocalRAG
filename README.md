# Local RAG System

A local Retrieval-Augmented Generation system built with Python, PostgreSQL, Elasticsearch, and Ollama.

## Features

- **Database-Backed Storage**: Documents and chunks stored in PostgreSQL with pgvector
- **Vector Search**: High-performance similarity search using Elasticsearch with dense vectors
- **Hybrid Retrieval**: Combine vector similarity with BM25 text search
- **Advanced Document Processing**: Docling-powered parsing with layout awareness and table extraction
- **Multi-Format Support**: Load documents from .txt, .pdf, .docx, .pptx, .xlsx files with unified processing
- **Multilingual Support**: Automatic language detection and processing for English, German, French, and Spanish
- **Ollama Integration**: Local LLM generation with context from retrieved documents
- **Web Interface**: Modern Streamlit UI for querying, document management, and analytics
- **Scalable Architecture**: Designed for production use with proper database indexing

## Setup

1. **Clone or navigate to the project directory**

2. **Create virtual environment:**
   ```bash
   python3 -m venv rag_env
   source rag_env/bin/activate  # On Windows: rag_env\Scripts\activate
   ```

3. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

4. **Set up databases:**
    - **Option 1 - Docker Compose (Recommended):**
      ```bash
      python setup_databases.py docker
      ```
      Or manually: `docker-compose up -d`

      This starts both PostgreSQL (with pgvector) and Elasticsearch with proper configuration.

    - **Option 2 - Local Setup:**
      ```bash
      python setup_databases.py local
      ```
      Follow the printed instructions for local PostgreSQL and Elasticsearch installation.

    - Update `.env` file with database credentials (see comments in `.env` for Docker vs local settings)

5. **Initialize databases:**
    ```bash
    python scripts/migrate_to_db.py  # Process documents and create chunks/embeddings
    python src/database/opensearch_setup.py  # Set up Elasticsearch indices
    ```

6. **Install and setup Ollama:**
   - Download from https://ollama.ai
   - Pull a model: `ollama pull llama2`
   - Start server: `ollama serve`

## Usage

### Web Interface (Recommended):
```bash
python run_web.py
```
Or directly:
```bash
streamlit run web_interface/app.py
```
Then open http://localhost:8501 in your browser for a comprehensive multipage experience with:
- **ğŸ  Home**: Query interface with RAG and retrieval-only modes
- **ğŸ“ Documents**: Upload and manage documents with automatic processing
- **âš™ï¸ Settings**: Configure generation parameters and interface options
- **ğŸ“Š Analytics**: Monitor system performance and query statistics

### Command Line Interface:
```bash
python -m src.app
```

Choose between:
- **Retrieval only**: Search for relevant documents
- **Full RAG**: Get AI-generated answers with context

### Testing:
```bash
python test_system.py  # Run system tests
python test_performance.py  # Performance benchmarking
```

## Project Structure

```
LocalRAG/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ app.py              # CLI interface
â”‚   â”œâ”€â”€ data_loader.py      # Document loading and chunking
â”‚   â”œâ”€â”€ document_processor.py # Database document processing
â”‚   â”œâ”€â”€ embeddings.py       # Embedding creation utilities
â”‚   â”œâ”€â”€ rag_pipeline_db.py  # RAG pipeline with database
â”‚   â”œâ”€â”€ retrieval_db.py     # Database-backed retrieval
â”‚   â””â”€â”€ database/
â”‚       â”œâ”€â”€ models.py       # SQLAlchemy models
â”‚       â”œâ”€â”€ opensearch_setup.py # Elasticsearch configuration
â”‚       â””â”€â”€ schema.sql      # Database schema
â”œâ”€â”€ web_interface/
â”‚   â”œâ”€â”€ app.py              # Main Streamlit app
â”‚   â”œâ”€â”€ pages/              # Individual pages
â”‚   â””â”€â”€ components/         # Reusable UI components
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ migrate_to_db.py    # Database migration script
â”œâ”€â”€ setup_databases.py      # Database setup helper script
â”œâ”€â”€ test_system.py          # System tests
â”œâ”€â”€ test_performance.py     # Performance benchmarks
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ docker-compose.yml      # Docker services configuration
â”œâ”€â”€ plan.md                 # Implementation plan
â””â”€â”€ README.md               # This file
```

## Architecture

- **Document Processing**: Documents are parsed using Docling for superior text extraction, then chunked and embedded using nomic-embed-text-v1.5
- **Storage**: Chunks and metadata stored in PostgreSQL, embeddings indexed in Elasticsearch
- **Retrieval**: Hybrid search combining vector similarity and BM25 text search
- **Generation**: Context from retrieved documents fed to Ollama LLMs for answer generation

## Requirements

- Python 3.8+
- Docker (recommended for databases) OR:
  - PostgreSQL with pgvector extension
  - Elasticsearch 8.x
- Ollama for local LLM inference

## Implementation Status

See `plan.md` for detailed implementation progress. The system is fully operational with database-backed storage and vector search.