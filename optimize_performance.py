#!/usr/bin/env python3
"""
Performance optimization script for the Local RAG system.
"""

import sys
from pathlib import Path

# Add src directory to path
sys.path.insert(0, str(Path(__file__).parent / 'src'))

from src.rag_pipeline_db import RAGPipelineDB

def optimize_system():
    """Apply performance optimizations to the RAG system."""
    print("‚ö° Local RAG Performance Optimization")
    print("=" * 50)

    # Initialize RAG pipeline with optimizations
    print("üöÄ Initializing optimized RAG pipeline...")

    # Enable batch processing and caching
    rag_pipeline = RAGPipelineDB(
        llm_model="llama3.2:latest",
        cache_enabled=True,
        cache_settings={
            'host': 'localhost',
            'port': 6379,
            'ttl_hours': 24
        }
    )

    # Start batch processing
    print("üì¶ Starting batch embedding processing...")
    rag_pipeline.retriever.start_batch_processing()

    # Test batch processing
    batch_stats = rag_pipeline.retriever.get_batch_stats()
    if batch_stats:
        print("‚úÖ Batch processing enabled:")
        print(f"   ‚Ä¢ Device: {batch_stats.get('device', 'unknown')}")
        print(f"   ‚Ä¢ Batch size: {batch_stats.get('max_batch_size', 0)}")
        print(f"   ‚Ä¢ Status: {'Running' if batch_stats.get('is_running') else 'Stopped'}")
    else:
        print("‚ö†Ô∏è Batch processing not available")

    # Test cache
    if rag_pipeline.cache_enabled and rag_pipeline.cache:
        print("üíæ Redis cache enabled and connected")
        cache_stats = rag_pipeline.cache.get_stats()
        if cache_stats:
            print(f"   ‚Ä¢ Memory used: {cache_stats.get('memory_used', 'unknown')}")
            print(f"   ‚Ä¢ Hit rate: {cache_stats.get('hit_rate', 0):.1%}")
            print(f"   ‚Ä¢ Total keys: {cache_stats.get('total_keys', 0)}")
    else:
        print("‚ö†Ô∏è Redis cache not available")

    print("\nüéØ Performance Optimizations Applied:")
    print("   ‚úÖ Batch embedding processing enabled")
    print("   ‚úÖ Redis caching enabled")
    print("   ‚úÖ Optimized LLM model (llama3.2:latest)")
    print("   ‚úÖ Knowledge graph contextual expansion")

    print("\nüìä Recommended Next Steps:")
    print("   ‚Ä¢ Monitor query latency in analytics dashboard")
    print("   ‚Ä¢ Check cache hit rates for optimization opportunities")
    print("   ‚Ä¢ Consider GPU acceleration for larger deployments")
    print("   ‚Ä¢ Review batch processing stats for embedding performance")

if __name__ == "__main__":
    optimize_system()